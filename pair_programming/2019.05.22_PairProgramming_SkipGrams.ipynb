{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pair Problem  \n",
    "\n",
    "This should be fun. Try to use sklearn to implement SkipGrams using the sklearn.neural_network.MLPClassifier class.\n",
    "\n",
    "Remember:  \n",
    "\n",
    "The input to SkipGrams is a one-hot encoded vector for the word under examination in a context window.\n",
    "The output is the similar one-hot vectors for the remaining words in the window\n",
    "\n",
    "Effectively:\n",
    "\n",
    "You can treat the outputs as if it were a multiclass, multilabel problem. That is, each training observation (a context window) will have a 1-hot encoded vector as input (representing the middle word in the window) and 4 integers as output (representing the \"classes\" for that observation, with the classes here being the indexes of the context words in your vocabulary).  \n",
    "With this formulation, you should be able to make it work with the MLPClassifier, although start with very small amounts of data.\n",
    "\n",
    "Data:  \n",
    " - Use the 20 newsgroups data from sklearn but only take a small sample of the data. word2vec is not quick.\n",
    "\n",
    "Suggested Steps:\n",
    "\n",
    " - Set a Window size (the number of words before and after the target word included in the context window, I suggest 2 for now)\n",
    "\n",
    " - Use a CountVectorizer to establish the term indexes in your vocabulary\n",
    "\n",
    " - For each context window, use the CountVectorizer to get the appropriate 1-hot vector as input and 4 integers (labels) as output (representing the 4 context words for a window size of 2)\n",
    "\n",
    " - Plug in your observations (X=1-hot vector, and y=vector of 4 integers) and fit an MLPClassifier and see how it does!\n",
    "\n",
    " - That is, you need to use the coef_ (the weights matrix W) attribute from your CountVectorizer to map the word vectors into the new word2vec space. The transformation for this remember, is W'x where W' is the transpose of W and x is the 1-hot vector for a word.\n",
    "\n",
    " - Compute some cosine similarities between a few terms using the new W'x vectors. Any luck at all?\n",
    "\n",
    "Feel free to collaborate extensively on this one. I don't want it to take too too long but I understand it is involved.\n",
    "\n",
    "May the best vectors win!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a Window size (the number of words before and after the target word included \n",
    "# in the context window, I suggest 2 for now)\n",
    "\n",
    "# Set Network Parameters\n",
    "window_size = 2\n",
    "dim = 100 #dimensions of hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Data\n",
    "ng = fetch_20newsgroups().data[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a CountVectorizer to establish the term indexes in your vocabulary\n",
    "cv = CountVectorizer()\n",
    "ng_vecs = cv.fit_transform(ng)\n",
    "\n",
    "# Store those indices here\n",
    "vocab = cv.vocabulary_ #word to index\n",
    "\n",
    "# And the reverse mapping\n",
    "id2word = {v:k for (k,v) in vocab.items()} #reversed mapping of index to word (reverse of vocab)\n",
    "\n",
    "# The total unique words, aka size of vocabulary\n",
    "V = len(vocab) #Output layer size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CountVectorizer to turn our list of documents into a Series of lists of terms\n",
    "tokenizer = cv.build_tokenizer()\n",
    "tokenized_docs = pd.Series(ng).map(tokenizer).map(lambda x: [a.lower() for a in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['from',\n",
       " 'lerxst',\n",
       " 'wam',\n",
       " 'umd',\n",
       " 'edu',\n",
       " 'where',\n",
       " 'my',\n",
       " 'thing',\n",
       " 'subject',\n",
       " 'what']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_docs[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the X data matrix and y vector for MLP\n",
    "# X: A matrix of one-hot encoded vectors (dimension V) for each center word over all context windows (size 2+2+1=5)\n",
    "# y: A matrix over all context windows where the outputs are the 4 \"labels\", aka the indices of the 4 \"other\" context words\n",
    "X = []\n",
    "y = []\n",
    "# Step thru tokenized document list\n",
    "for doc in tokenized_docs:\n",
    "    # For each document, step thru the words\n",
    "    for index, word in enumerate(doc): \n",
    "        # Skip if at the edge of a document (can handle differently)\n",
    "        if index < 2 or index > (len(doc)-3):\n",
    "            continue\n",
    "        # Retrieve the one-hot V-dimensional input vector and add it to X\n",
    "        one_hot_input = [0]*V \n",
    "        one_hot_input[vocab[word]] = 1\n",
    "        # HACK: Had to do the window cooccurrences separately as MLP won't support multilabel tho it says it does\n",
    "        X.append(one_hot_input)\n",
    "        X.append(one_hot_input)\n",
    "        X.append(one_hot_input)\n",
    "        X.append(one_hot_input)\n",
    "        # Retrieve the 4 outputs for the context window and add them to y\n",
    "        # Same HACK here\n",
    "        context1 = doc[index-2]\n",
    "        y.append(vocab[context1])\n",
    "        context2 = doc[index-1]\n",
    "        y.append(vocab[context2])\n",
    "        context3 = doc[index+1]\n",
    "        y.append(vocab[context3])\n",
    "        context4 = doc[index+2]\n",
    "        y.append(vocab[context4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50112, 3612)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### CHECK THIS, SOMETHING WRONG HERE (taking too long to run)\n",
    "\n",
    "# Fit the MLP Classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(dim,))\n",
    "mlp.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the word vectors!!\n",
    "word_vecs = mlp.coefs_[0]\n",
    "word_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the closest words to a query using a K-Nearest Neighbors search with cosine\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nn = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "nn.fit(word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the closest words to a query using a K-Nearest Neighbors search with cosine\n",
    "def get_similar(query, n=10):\n",
    "    query_index = vocab[query]\n",
    "    if query_index:\n",
    "        dist, index = nn.kneighbors(word_vecs[query_index, :], n_neighbors=n)\n",
    "        return ([(id2word[i[0]], d[0]) for (d, i) in zip(dist.transpose(), index.transpose())])\n",
    "    else:\n",
    "        return \"Query not in the dataset!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it out!\n",
    "get_similar(\"bat\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
